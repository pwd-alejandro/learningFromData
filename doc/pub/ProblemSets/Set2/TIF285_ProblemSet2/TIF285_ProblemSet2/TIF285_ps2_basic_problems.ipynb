{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- See deadline on the course web page\n",
    "- This problem set is performed individually. See examination rules on the course web page.\n",
    "- Students are allowed to discuss together and help each other when solving the problems. However, every student must understand and be able to explain his/her submitted solution. Plagiarism is not allowed (submissions will be both manually and automatically monitored).\n",
    "- The two notebooks for each problem set contain a number of basic and extra problems; you can choose which and how many to work on.\n",
    "- Many problems are automatically graded using `assert` statements. You should check that your code passes these statements without raising an `AssertionError`. Note that there might be additional, hidden tests that must be passed for full credit. In addition, some tasks are either completely manually graded or part-automatic/part-manual.\n",
    "- Note that grading is performed in the teacher's python environment, which is based on the conda `environment.yml` file in the course github repo. Please avoid using additional python modules (such as `plotly`) as this might cause automatic tests to fail.\n",
    "\n",
    "- **Important:** Hand-in is performed through the following actions:\n",
    "  - Make sure to always complete **Task 0** in the header part of the notebook and that this part does not raise any `AssertionError`(s). \n",
    "  - Upload your solution in the form of your edited version of this jupyter notebook via the appropriate module in Canvas.\n",
    "  - The name of the uploaded file **must be the same as the original one**!\n",
    "  \n",
    "  Note that the hand-in might not be automatically graded if you have changed the name of the uploaded file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "- Make sure that the **run time is smaller than a few minutes**. If needed you might have to reduce some computational tasks; e.g. by decreasing the number of grid points or sampling steps. Please ask the supervisors if you are uncertain about the run time. \n",
    "\n",
    "- Your solutions are usually expected where it says `YOUR CODE HERE` or <font color=\"red\">\"PLEASE WRITE YOUR ANSWER HERE\"</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0 \n",
    "#### (0 points)\n",
    "Fill your personal details in the dictionary 'student' with the following key/value pairs:\n",
    "- **Lastname**: Your lastname as a string\n",
    "- **Firstname**: Your firstname as a string\n",
    "- **DOB-year**: The year for your date of birth as a four-digit integer\n",
    "- **DOB-month**: The month for your date of birth as an integer (1-12)\n",
    "- **DOB-day**: The year for your date of birth as an integer (1-31)\n",
    "- **CID**: Your Chalmers login ID as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e5a863952e8c2244ba81d9a82755064",
     "grade": false,
     "grade_id": "student_info",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "student={}\n",
    "# Update the values below. Note the formats.\n",
    "student['Lastname']='Lozada'  # string\n",
    "student['Firstname']='Alejandro' # string\n",
    "student['CID']='lozadaa'        # string\n",
    "student['DOB-year']=1997         # four-digit integer\n",
    "student['DOB-month']=4           # integer in the range [1, 12]\n",
    "student['DOB-day']=17             # integer in the range [1, 31]\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04fad85b593dac43e9a577b7ad0eda92",
     "grade": true,
     "grade_id": "correct_student_info",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for key in ['Lastname', 'Firstname','CID']:\n",
    "    assert type(student[key]) is str, f'{key} is wrong type.'\n",
    "    assert student[key] not in ['name_here','cid_here'],\\\n",
    "        f'Fill your {key} as a string.'\n",
    "\n",
    "for key in ['DOB-year', 'DOB-month','DOB-day']:\n",
    "    assert type(student[key]) is int, f'{key} is wrong type.'\n",
    "\n",
    "assert (student['DOB-year'] > 1900 and student['DOB-year'] < 2100)\n",
    "assert student['DOB-month'] in range(1,13), \\\n",
    "    'DOB month should be an integer in the range [1, 12]'\n",
    "assert student['DOB-day'] in range(1,32), \\\n",
    "    'DOB day should be an integer in the range [1, 31]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e21e909aaacb19cde0a49f423690276",
     "grade": false,
     "grade_id": "cell-6f99a2583f9fb27d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By changing the below boolean variable `student_self_assessment` to `True` you attest that:\n",
    "1. All handed in solutions were produced by yourself in the sense that you understand your solutions and should be able to explain and discuss them with a peer or with a teacher.\n",
    "2. That discussions with your peers are allowed, also concerning approaches to solve the problem sets, but that direct plagiarism is not allowed and that you must reach your own understanding of submitted solutions according to the definition in the previous point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3246da2eb074f5c38175d68ad158c48e",
     "grade": false,
     "grade_id": "student-self-assessment",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "student_self_assessment = True\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d873afed15d2d3de2ef460d53fccf90f",
     "grade": true,
     "grade_id": "cell-795bedd2908899aa",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert student_self_assessment == True, 'You must assert the individual solution statements.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2\n",
    "## Basic problems\n",
    "### Learning from data [TIF285], Chalmers, Fall 2022\n",
    "\n",
    "Last revised: 12-Sep-2022 by Christian ForssÃ©n [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c532e9c42f0ac2234f7eb5f88c73672b",
     "grade": false,
     "grade_id": "cell-f3768fbcb502fd03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Data files are stored in\n",
    "DATA_DIR = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Coin tossing\n",
    "### (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data with simulated coin tosses from the file `cointosses.dat`.\n",
    "Each row corresponds to a single toss: 0=tails; 1=heads\n",
    "\n",
    "Extract the mean and the 95% credible intervals (Degree-of-belief or DoB intervals) from the first 8 tosses, the first 64 tosses, the first 512 tosses and all 4096 tosses in the data assuming a uniform prior for the probability $p_H$ of obtaining a head in a single toss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint*: Sample code for computing the DoB interval is available in the demonstration notebook `demo-BayesianBasics.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfac07e5196a5338c2f1dd73aaf99afd",
     "grade": false,
     "grade_id": "cell-f27c4016e4570c15",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# importing modules\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f47480c3ca8ec761da96629640e5ea8",
     "grade": false,
     "grade_id": "cell-71d9ceb95f1f80db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = np.loadtxt(f'{DATA_DIR}/cointosses.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09fb7549969e9902c830ddf74deba99e",
     "grade": false,
     "grade_id": "cell-79bec52ebb8e7d12",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Optional. \n",
    "# Insert utility code / functions here.\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     11,
     15
    ],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1dc5a85564a7cdfbfc8a0413131abc4",
     "grade": false,
     "grade_id": "cell-538f840bea75b7dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function that returns the mean, median, and 68%/95% credible intervals \n",
    "# of the Bayesian posterior with an input data array of coin flips \n",
    "# and using a uniform [0,1] prior for the pH probability of getting a head result.\n",
    "\n",
    "def bayesian_analysis_coin_flips(data_coin_tosses):\n",
    "    \"\"\"\n",
    "    Returns various Bayesian analysis results for the given data of coin tosses.\n",
    "    \n",
    "    The posterior is p( pH | data, I).\n",
    "    Assume a uniform p(pH|I) = U[0,1] prior\n",
    "    \n",
    "    Args:\n",
    "        data_coin_tosses: Array of shape (m,) with 'm' independent binary data.\n",
    "            0 = tails; 1 = heads\n",
    "            \n",
    "    Returns:\n",
    "        (mean, mode, median, dob68, dob95): A tuple with the following elements\n",
    "            mean: The mean of the posterior distribution (float)\n",
    "            mode: The mode of the posterior distribution (float)\n",
    "            median: The median of the posterior distribution (float)\n",
    "            dob68: A tuple (lo,hi) with the lower and upper limits of the \n",
    "                68% degree-of-belief range of the posterior distribution (float,float)\n",
    "            dob95: A tuple (lo,hi) with the lower and upper limits of the \n",
    "                95% degree-of-belief range of the posterior distribution (float,float)\n",
    "    \"\"\"\n",
    "    heads=np.sum(data_coin_tosses)\n",
    "    N=len(data_coin_tosses)\n",
    "    \n",
    "    #assuming binomial distribution\n",
    "    dist = stats.beta(1+heads,1+N-heads)\n",
    "    \n",
    "    median = dist.median()\n",
    "    mean = dist.mean()\n",
    "    mode = data_coin_tosses[dist.pdf(data_coin_tosses).argmax()]\n",
    "    \n",
    "    dob68 = dist.interval(0.68)\n",
    "    dob95 = dist.interval(0.95)\n",
    "    \n",
    "    return (mean, mode, median, dob68, dob95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e70da91a91ae0051ded00cd9dd50b27",
     "grade": true,
     "grade_id": "cell-a567356cf18d3da5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "(mean, mode, median, dob68, dob95) = bayesian_analysis_coin_flips(data[:1])\n",
    "for output in (mean, mode, median, dob68[0], dob95[0]):\n",
    "    assert output.dtype=='float64', 'Wrong type'\n",
    "assert len(dob68)==2, 'DoB tuple should be of length 2'\n",
    "assert len(dob95)==2, 'DoB tuple should be of length 2'\n",
    "assert np.abs(mean-0.667)<0.001\n",
    "assert np.abs(mode-1.0)<0.001\n",
    "assert np.abs(median-0.707)<0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Problem 2: Straight line fitting\n",
    "### (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will be fitting a straight line to a set data. Our model has two parameters $\\theta=[b,m]$ (pay attention to the indexing $\\theta[0]=b; \\theta[1]=m$)\n",
    "\n",
    "$$\n",
    "y_M(x) = mx + b\n",
    "$$\n",
    "\n",
    "And our statistical model assumes that errors are normally distributed\n",
    "\n",
    "$$\n",
    "y_i = y_M(x_i;\\theta) + \\varepsilon_i,\n",
    "$$\n",
    "\n",
    "where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ and we assume a fixed standard deviation $\\sigma = 50$. \n",
    "\n",
    "(Note that the $\\varepsilon_i \\sim \\ldots$ notation means that $\\varepsilon_i$ is a random variable that follows the specified distribution.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read the data from the file `straightline.dat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91e7025b33e18c3cbb00b5158090d2ac",
     "grade": false,
     "grade_id": "cell-0609945db7cac2cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the data and plot with fixed error bar \n",
    "# Use np.loadtxt() for loading data (the argument 'unpack=True' is useful)\n",
    "# and plt.errorbar() for plotting data with errorbars\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use a flat prior for the intercept:\n",
    "- $-1000 < b < 1000$\n",
    "\n",
    "For the slope parameter we try with two different priors : \n",
    "1. a flat prior, $-1000 < m < 1000$\n",
    "1. a symmetric / scale-invariant one.\n",
    "\n",
    "In both cases we return the log of the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14af05cd07ff6d47d416777bacb30c13",
     "grade": false,
     "grade_id": "cell-85c54c4a189fc811",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_flat_prior(theta):\n",
    "    if np.all(np.abs(theta) < 1000):\n",
    "        return 0 # log(1)\n",
    "    else:\n",
    "        return -np.inf  # log(0)\n",
    "    \n",
    "def log_symmetric_prior(theta):\n",
    "    if np.abs(theta[0]) < 1000:\n",
    "        return -1.5 * np.log(1 + theta[1] ** 2)\n",
    "    else:\n",
    "        return -np.inf  # log(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These are some questions that will be addressed in this problem:\n",
    "* Where is the mode of the posterior with these two different priors?\n",
    "* Plot the joint pdf for the slope and the intercept for the two different prior choices.\n",
    "* Are the two parameters correlated or anticorrelated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "890569c314d997f6ebe60e57fa81b469",
     "grade": false,
     "grade_id": "cell-958720f73ee427a3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x, y, dy=50):\n",
    "    '''\n",
    "    Returns the log likelihood.\n",
    "    \n",
    "    Args:\n",
    "        theta: array of floats with two elements. theta[0]=intercept. theta[1]=slope\n",
    "        x: data (independent variable). array of floats\n",
    "        y: data (dependent variable). array of floats\n",
    "        dy: fixed error (optional; default=50), standard deviation of a normal distribution\n",
    "        \n",
    "    Returns:\n",
    "        logL: log likelihood\n",
    "    '''\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "823a8d7ecf1ccd97af674d1192c34efa",
     "grade": false,
     "grade_id": "cell-ba8d7bc36ae04469",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We'll start by defining a function which takes a two-dimensional grid of likelihoods and \n",
    "# returns 1, 2, and 3-sigma contours. This acts by sorting and normalizing the values and then \n",
    "# finding the locations of the  0.682 ,  0.952 , and  0.9972  cutoffs:\n",
    "def contour_levels(grid):\n",
    "    \"\"\"Compute 1, 2, 3-sigma contour levels for a gridded 2D posterior\"\"\"\n",
    "    _sorted = np.sort(grid.ravel())[::-1]\n",
    "    pct = np.cumsum(_sorted) / np.sum(_sorted)\n",
    "    cutoffs = np.searchsorted(pct, np.array([0.68, 0.95, 0.997]) ** 2)\n",
    "    return np.sort(_sorted[cutoffs])\n",
    "\n",
    "# Optional. \n",
    "# Insert utility code / functions here.\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0aa503171650b39c0ec663f698f00a79",
     "grade": false,
     "grade_id": "cell-97243b6cddebf189",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The dictionary MAP (= maximum a posteriori) should return the mode \n",
    "# of the posterior distribution.\n",
    "# The key is the prior and the value is resulting posterior mode (peak)\n",
    "# given as theta* = [b*, m*]\n",
    "MAP={}\n",
    "MAP['uniform_prior'] = [0.0, 0.0]\n",
    "MAP['symmetric_prior'] = [0.0, 0.0]\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e72d4049cf6dd4cc1d08751f5fee09c",
     "grade": true,
     "grade_id": "cell-e706e9b2ce0c89d9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for prior in ['uniform_prior','symmetric_prior']:\n",
    "    assert len(MAP[prior])==2, f'{prior}: The length of the MAP mode should be 2.'\n",
    "    assert MAP[prior][0] != 0.0, f'{prior}: The intercept should not be 0.0.'\n",
    "    assert MAP[prior][1] != 0.0, f'{prior}: The slope should not be 0.0.'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e1545461ec1e730d8f98e73236d2de5",
     "grade": false,
     "grade_id": "cell-9e84255bf61c598d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "- Plot the joint posterior for the two model parameters for the two different priors.\n",
    "- Indicate whether the slope and the intercept are correlated or anti-correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1299570d5496455de42d9a1a74851bec",
     "grade": false,
     "grade_id": "cell-80f2e16a2e4b7897",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Problem 3: MCMC sampling of a Lorentzian pdf using the random walk Metropolis algorithm\n",
    "### (3 points)\n",
    "Note that you must solve this problem if you want to solve (extra) problem 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Say that we have some function that tells us the (possibly unnormalized) probability of a given position in a one-dimensional space. Note, however, that a key feature of the approach that we will implement here is that it can be extended to many dimensions. \n",
    "\n",
    "We will assume a known, specific form of this univariate pdf, namely a Lorentzian (Cauchy) distribution, but it might just as well be some very complicated function that can only be evaluated numerically. All that is needed is some function that, for each position in the parameter space, returns a number that is proportial to the probability density.\n",
    "\n",
    "Let us start by studying the pdf that we will be sampling from using a random walk (using the Metropolis algorithm outlined below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4059932e0316adaa77984611fd281d25",
     "grade": false,
     "grade_id": "cell-39860bc3703e35b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Modules needed for this exercise\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import cauchy\n",
    "# used for plotting \n",
    "import seaborn as sns\n",
    "sns.set('talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Draw a number of random samples from the standard Cauchy\n",
    "r = cauchy.rvs(size=1000)\n",
    "plt.hist(r, density=True, histtype='stepfilled', alpha=0.2, \n",
    "         range=(-10,10),bins=21);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This histogram corresponds to a finite sample from the pdf of a standard Cauchy (Lorentzian)\n",
    "$$ \n",
    "p(x | \\alpha=0, \\beta=1) = \\frac{1}{\\pi(1+x^2)}, \n",
    "$$\n",
    "with mean $\\alpha=0$ and FWHM $2\\beta = 2$.\n",
    "\n",
    "Questions to ponder (not part of this problem; need not be answered in the notebook):\n",
    "- How does this pdf compare with a standard normal distribution $\\mathcal{N}(x;\\mu=0,\\sigma^2=1)$?\n",
    "- The Cauchy distribution is often used in statistics as the canonical example of a \"pathological\" distribution since both its mean value and its variance are undefined. Do you see mathematically why these moments are undefined?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, turn the posterior into a callable function. You should deliberately remove the normalization to make the point that sampling can be made for an unnormalized pdf. Note that we will work directly with the pdf here (not taking the log as in previous examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9dc856be50fec682e87ff38ced25e84",
     "grade": false,
     "grade_id": "cell-16bc7d26f9be4be2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def posterior_function(x, normalized=False):\n",
    "    '''\n",
    "    Return the posterior pdf given by a standard Cauchy (Lorentzian).\n",
    "    \n",
    "    Args:\n",
    "        x: position in a one-dimensional space\n",
    "        normalized: Return a normalized pdf if True (optional, default=False)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27b8474a56d46b80f193b9b09f1fbb47",
     "grade": false,
     "grade_id": "cell-027ae99a7ad7d43a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Predefined function for visualization.\n",
    "# No changes are needed.\n",
    "# The distplot method is a deprecated function and will be removed in a future version.\n",
    "# Remove warning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def plot_proposal(posterior_func, current_position, p_current, \n",
    "                  proposed_position, p_proposal, accepted, trace, i):\n",
    "    from copy import copy\n",
    "    trace = copy(trace)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "    fig.suptitle('Iteration %i' % (i + 1),fontsize=14)\n",
    "    x = np.linspace(-5, 5, 5000)\n",
    "    color = 'g' if accepted else 'r'\n",
    "    label_text = 'Accepted' if accepted else 'Rejected'\n",
    "        \n",
    "    # Plot posterior\n",
    "    ax1.plot(x, posterior_func(x))\n",
    "    ax1.plot([current_position] * 2, [0, p_current], marker='o', color='b')\n",
    "    ax1.plot([proposed_position] * 2, [0, p_proposal], marker='o', color=color)\n",
    "    ax1.annotate(\"\", xy=(proposed_position, 0.2), xytext=(current_position, 0.2),\n",
    "                 arrowprops=dict(arrowstyle=\"->\", lw=2.))\n",
    "    ax1.set(ylabel='Probability Density', \\\n",
    "            title=f'current: posterior(x={current_position:.2f}) = {p_current:.2}\\n'+\\\n",
    "            f'proposal: posterior(x={proposed_position:.2}) = {p_proposal:.2}')\n",
    "    \n",
    "    if accepted:\n",
    "        trace.append(proposed_position)\n",
    "    else:\n",
    "        trace.append(current_position)\n",
    "        \n",
    "    # Posterior histogram\n",
    "    ax2.plot(x, posterior_function(x, normalized=True)) # properly normalized\n",
    "    sns.distplot(trace, kde=False, norm_hist=True, ax=ax2)\n",
    "    ax2.axvline(current_position, color='b', linestyle='--', \n",
    "                label='current position')\n",
    "    ax2.axvline(proposed_position, color=color, linestyle='--', \n",
    "                label='proposed position')\n",
    "    ax2.annotate(\"\", xy=(proposed_position, 0.2), xytext=(current_position, 0.2),\n",
    "                 arrowprops=dict(arrowstyle=\"->\", lw=2.))\n",
    "    ax2.annotate(label_text, xy=(proposed_position, 0.5), color=color,rotation=270,fontsize=14)\n",
    "\n",
    "    \n",
    "    ax3.plot(trace)\n",
    "    ax3.set(xlabel='iteration', ylabel='position', title='trace')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now on to the sampling. The code for a MCMC sampler that uses the Metropolis algorithm is enclosed below. However, it misses a few critical ingredients and it is your task to add them at the correct places.\n",
    "\n",
    "1. At first, you have to specify the initial parameter position (that can be randomly chosen), lets fix it to the input argument `start_position`:\n",
    "\n",
    "```python\n",
    "current_position = start_position\n",
    "```\n",
    "\n",
    "Then, you propose to move (jump) to another position. You can be very dumb or very sophisticated about how you come up with the proposed jump. The Metropolis sampler is rather dumb and just picks a sample from a symmetric proposal distribution (here we again choose a normal distribution) centered around your current position (i.e. `current_position`) with a certain standard deviation (`proposal_width`) that will determine the length scale for proposed jumps \n",
    "\n",
    "2. Use `scipy.stats.norm` to create `proposed_position` as a random variable that has the mean value equal to the `current_position` and a standard deviation that is given by `proposal_width`.\n",
    "\n",
    "Next, you evaluate whether that new position is indeed a good place to jump to, or not. We quantify this by comparing the probability density at the `proposed_position` in parameter space with the one for the `current_position`. Usually you would use the logarithms of the probability densities but we omit this here.\n",
    "\n",
    "3. Compute both `p_current` and `p_proposal`.\n",
    "\n",
    "Up until now, we essentially have a hill-climbing algorithm that would just propose movements into random directions and only accept a jump if the `proposed_position` has higher probability density than `current_position`. Eventually we'll get to `x = 0` (or close to it) from where no more moves will be accepted. However, we want to get samples from a pdf so we'll also have to sometimes accept moves into regions of lower probability. The key trick is to divide the two probabilities,\n",
    "\n",
    "```python\n",
    "p_accept = p_proposal / p_current\n",
    "```\n",
    "\n",
    "and to interpret this ratio as an acceptance probability. Note that the acceptance probability is obtained by dividing the pdf of the proposed parameter setting by the pdf of the current parameter setting. This implies that the probability density does not necessarily need to be nomalized, the normalization factor will anyway cancel out. \n",
    "\n",
    "You can see that if `p_proposal` is larger, the acceptance probability will be `> 1` and we'll definitely accept the jump. However, if `p_current` is larger, say twice as large, there'll be a 50% chance of moving there, which we will decide by drawing a random number.\n",
    "\n",
    "4. Add the acceptance step by comparing `p_accept` to a random number (uniform [0,1]). The `current_position` should be updated if the `accept` variable is `True`. \n",
    "\n",
    "Note that the `current_position` is added to our list of parameter samples at the end of the iteration, regardless of it being a new position or not.\n",
    "\n",
    "This simple procedure gives us samples from the pdf.\n",
    "\n",
    "The code below also calls a fancy visualization function `plot_proposal` if the optional keyword argument `plot=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03634202bcec6a27496ee0aed32c596a",
     "grade": false,
     "grade_id": "cell-30d4e414d6f57b40",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sampler(posterior_func, no_of_samples=4, start_position=.5, \n",
    "            proposal_width=1., plot=False):\n",
    "    '''\n",
    "    Simple (but incomplete) Metropolis sampler function.\n",
    "    \n",
    "    Args:\n",
    "        posterior_func: Function that takes a single positional argument and returns \n",
    "            the (possibly unnormalized) pdf value.\n",
    "        no_of_samples: (integer) Number of samples that will be returned (excluding the start position). \n",
    "            (default=4)\n",
    "        start_position: (float) Start position. (default=0.5)\n",
    "        proposal_width: (float) Width of Gaussian proposal distribution. (default=1.)\n",
    "        plot: (Boolean) Make visualization (default=False)\n",
    "        \n",
    "    Returns:\n",
    "        samples: Array of floats. Length = no_of_samples+1\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    samples = [current_position]\n",
    "    for i in range(no_of_samples):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Compute posteriors of current and proposed position       \n",
    "        p_current = posterior_func(current_position)\n",
    "        p_proposal = posterior_func(proposed_position) \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Visualization\n",
    "        if plot:\n",
    "            assert no_of_samples < 11, \"Too many samples for visualization\"\n",
    "            plot_proposal(posterior_func, current_position, p_current, \n",
    "                          proposed_position, p_proposal, accept, samples, i)\n",
    "        \n",
    "        # Possibly update position\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        samples.append(current_position)\n",
    "        \n",
    "    return np.array(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a39624afadb6e1e9d2d7a9c39ed5cb44",
     "grade": true,
     "grade_id": "cell-2ae464bbbc58c95f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2020)\n",
    "test_samples = sampler(posterior_function, no_of_samples=2)\n",
    "assert test_samples[0]==0.5, 'The first position sample should be the defalut start position = 0.5'\n",
    "assert np.abs(test_samples[1]-(-1.26884571))<0.001, 'The first sample given the default proposal'+\\\n",
    "    ', the correct posterior_function and the provided random seed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Visualization of Metropolis sampling procedure:\n",
    "To visualize the sampling, we'll create plots for some quantities that are computed. Each row below is a single iteration through our Metropolis sampler. These tests will also allow you to check whether your sampler works as expected.\n",
    "\n",
    "The first column displays our unnormalized posterior distribution. This is for visualization only, normally we would not be able to plot a nice curve to show the posterior. Here, we plug in our $x$ proposals. The vertical lines represent our current position in blue and our proposed position in either red or green (rejected or accepted, respectively). \n",
    "\n",
    "The 2nd column is our posterior distribution. Here we are displaying the normalized posterior as the blue curve compared to the normalized histogram of samples (green bars) and the move that was just proposed.\n",
    "\n",
    "The 3rd column is our trace (i.e. the posterior samples of visited positions that we're generating). Note that we store a sample at each iteration, irrespective of whether the proposal was accepted or rejected. In the latter situation, we keep the previous position and the line just stays constant.\n",
    "\n",
    "Note that we always accept moves to relatively more likely $x$ values (in terms of their posterior density), but only sometimes to relatively less likely $x$ values, as can be seen already in the first iteration, and later in iterations 6, 7, and 8 (the iteration number can be found at the top center of each row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "try:\n",
    "    samples = sampler(posterior_function, no_of_samples=5, start_position=.5, proposal_width=1., plot=True);\n",
    "except:\n",
    "    print('The method \"sampler\" must be defined and working as expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sampler(posterior_function, no_of_samples=5, start_position=.5, proposal_width=1., plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now the magic of MCMC is that you just have to do that for a long time, and the samples that are generated in this way come from the posterior distribution of your model. There is a rigorous mathematical proof that guarantees this which we won't go into detail here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ee087e1c93ca367cc5b865155326ba3",
     "grade": false,
     "grade_id": "cell-9a2da588961f37ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Final task:\n",
    "Draw 100,000 samples from your sampler and plot:\n",
    "* The trace (i.e. the sequence of draws of your single parameter x)\n",
    "* A normalized histogram of the samples compared to the true posterior pdf (normalized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a30bfa8e49901efaed73bb8fb68df4b",
     "grade": false,
     "grade_id": "cell-b7a2a59b1d878aa2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    samples = sampler(posterior_function, no_of_samples=100000, start_position=1.)\n",
    "except:\n",
    "    samples=None\n",
    "    print('The method \"sampler\" must be defined and working as expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5174892840c64591a553ec94aab1c9c9",
     "grade": false,
     "grade_id": "cell-17e4557f37667278",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plotting commands here\n",
    "#\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0e3c12c1d64ed15541654ac913928bd",
     "grade": true,
     "grade_id": "cell-02cd9d30ab8b4c8c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.abs(np.median(samples))<0.2, f'The median of the samples is {np.median(samples):.1}. It should be close to the median of the posterior pdf (=0.0)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Problem 4: Signal and background\n",
    "### (3 points)\n",
    "This problem is more demanding than the other basic problems. Note that you must solve this problem if you want to solve (extra) problem 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The goal of this problem is to estimate the amplitude of a signal when there is a background.  We'll take a limiting case where the background is flat, so it is completely specified by its magnitude $B > 0$, and the signal is known to be a Gaussian with unknown amplitude $A$ but known position (mean) and width (standard deviation). \n",
    "\n",
    "The measurements will be integer numbers of counts $\\{N_k\\}$ in well-defined (equally spaced) bins $\\{x_k\\}$. The index $k$ runs over integers labeling the bins.\n",
    "\n",
    "We can imagine three different goals of the data analysis:\n",
    "- Finding $A$ and $B$ given $\\{N_k\\}$.\n",
    "- Finding $A$ (we do not care about $B$).\n",
    "- Finding $B$ (we do not care about $A$).\n",
    "\n",
    "In all cases we consider the bin sizes and the signal shape (including its mean position and width) as known information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our statistical model includes the true signal plus a constant background. The signal and the background magnitudes are the unknown parameters while the other parameters dictating the signal (width $w$ and mean $x_0$ of the Gaussian) are known and fixed:\n",
    "\n",
    "$$\n",
    "   D_k = n_0 \\left[ A e^{-(x_k-x_0)^2/2 w^2} + B \\right]\n",
    "$$\n",
    "\n",
    "Here $n_0$ is a constant that scales with measurement time.  Note that $D_k$ is not an integer in general, unlike $N_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b98e7f9e54afebd4c78159acad1286a6",
     "grade": false,
     "grade_id": "cell-ade2557e6088a291",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import statements.\n",
    "# We use pickle to save and load a python dictionary\n",
    "import pickle\n",
    "# factorial from the math module is useful. You might consider other modules as well.\n",
    "from math import factorial\n",
    "\n",
    "# import additional modules as needed\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "644664b1809f5a1557ece48c4a674325",
     "grade": false,
     "grade_id": "cell-cea19416f4d5f062",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This function generates data according to the statistical model\n",
    "A_true = 1.\n",
    "B_true = 2.\n",
    "\n",
    "def exact_data(A, B, n_0, x_k, x_0=0., width=np.sqrt(5.)):\n",
    "    \"\"\"\n",
    "    Return the exact signal plus background.  The overall scale is n_0,\n",
    "    which is determined by how long counts are collected. \n",
    "    The default signal position and width are 0.0 and sqrt(5), respectively (in some  irrelevant units).\n",
    "    \"\"\"\n",
    "    return n_0 * (A * np.exp(-(x_k - x_0)**2/(2.*width**2)) + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Poisson distribution\n",
    "We are imagining a counting experiment, so the statistics of the counts we record will follow a Poisson distribution. It might be an interesting exercise to derive why this distribution is expected for a counting experiment. \n",
    "\n",
    "The Poisson discrete random variable from scipy.stats is defined by (see [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html))\n",
    "\n",
    "$$\n",
    "p(k \\mid \\mu) = \\frac{\\mu^k e^{-\\mu}}{k!} \\quad \\mbox{for }k\\geq 0 \\;.\n",
    "$$\n",
    "\n",
    "where $k$ is an integer and $\\mu$ is called the shape parameter. The mean and variance of this distribution are both equal to $\\mu$. Sivia and Gregory each use a different notation for for this distribution, which means you need to be flexible. \n",
    "\n",
    "For convenience, we'll define our own version in this notebook:\n",
    "\n",
    "$$\n",
    "p(N \\mid D) = \\frac{D^N e^{-D}}{N!} \\quad \\mbox{for }N\\geq 0 \\;.\n",
    "$$\n",
    "\n",
    "where $N$ is an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f44128714159ffde91418ff139a6f2b0",
     "grade": false,
     "grade_id": "cell-e10dd1c95952f91a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# make a dataset for exploring\n",
    "def make_dataset(A_true, B_true, width, x_0, databins=15, delta_x=1, D_max=100):\n",
    "    \"\"\"\n",
    "    Create a data set based on the number of bins (databins), the spacing\n",
    "    of bins (delta_x), and the maximum we want the exact result to have\n",
    "    (D_max, this fixes the n_0 parameter).\n",
    "    \n",
    "    Return arrays for the x points (xk_pts), the corresponding values of the\n",
    "    exact signal plus background in those bins (Dk_pts), the measured values\n",
    "    in those bins (Nk_pts, integers drawn from a Poisson distribution), the \n",
    "    maximum extent of the bins (x_max) and n_0.\n",
    "    \"\"\"\n",
    "    # set up evenly spaced bins, centered on x_0\n",
    "    x_max = x_0 + delta_x * (databins-1)/2\n",
    "    xk_pts = np.arange(-x_max, x_max + delta_x, delta_x, dtype=int)\n",
    "    \n",
    "    # scale n_0 so maximum of the \"true\" signal plus background is D_max\n",
    "    n_0 = D_max / (A_true + B_true)  \n",
    "    Dk_pts = exact_data(A_true, B_true, n_0, xk_pts, width=width)\n",
    "    \n",
    "    # sample for each k to determine the measured N_k\n",
    "    Nk_pts = [stats.poisson.rvs(mu=Dk) for Dk in Dk_pts]\n",
    "    \n",
    "    return xk_pts, Dk_pts, Nk_pts, x_max, n_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Plot the signal and the data (these tasks are not graded but will help you to understand the problem)\n",
    "* Make a plot of the true signal plus background we are trying to deduce. Use $A_\\mathrm{true}=1$ and $B_\\mathrm{true}=2$ and the signal position (mean) $x_0=0$ and width (standard deviation)  $w=\\sqrt{5}$.\n",
    "\n",
    "We consider what happens for fixed signal and background but changing the experimental conditions specified by `D_max` and `databins` (we'll keep `delta_x` fixed to 1). In all cases the bins are symmetric around $x=0$.\n",
    "\n",
    "The pickle file that is loaded in the cell below contains data from four differently designed counting experiments.:\n",
    "1. Baseline case: 15 bins and maximum expection of 100 counts per bin.\n",
    "1. Low statistics case: 15 bins and maximum expection of only 10 counts per bin.\n",
    "1. Greater range case: 31 bins (with fixed bin width) and maximum expection of 50 counts per bin to give approximately the same total number of counts as in baseline case.\n",
    "1. Smaller range case: 7 bins (with fixed bin width) and maximum expection of 200 counts per bin to give approximately the same total number of counts as in baseline case.\n",
    " \n",
    "* Make four subplots that correspond to the data from the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4d712e5d2b7de1daf4edc090e7358a1",
     "grade": false,
     "grade_id": "cell-b300bc6fd3ee1f07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the signal and background\n",
    "#\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# The data has been generated already and will be loaded from a pickle file.\n",
    "# It is a dictionary with four keys corresponding to the four cases, and each value\n",
    "# corresponding to xk_pts, Dk_pts, Nk_pts, x_max, n_0\n",
    "with open(f'{DATA_DIR}/data_signal_and_background.pickle','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    print('Loaded \"data\" dictionary from file.')\n",
    "    print('Extract data with:')\n",
    "    print('xk_pts, Dk_pts, Nk_pts, x_max, n_0 = data[case]')\n",
    "    print('where the key \"case\" is one of:')\n",
    "    cases = data.keys()\n",
    "    print(cases)\n",
    "\n",
    "# Plotting the data for the four cases\n",
    "#\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a937693bb335f107ff2802c595cb364f",
     "grade": false,
     "grade_id": "cell-d1a0b4b6a44f1f7b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Graded subtasks\n",
    "##### Subtask a. [2 points]\n",
    "* Implement functions for the (log) likelihood and for a uniform (log) prior. Let's use a uniform prior for $0 \\le A \\le 5$ and $0 \\le B \\le 5$.\n",
    "* Evaluate the log-posterior on a grid and then: \n",
    "  - Plot the joint posterior pdf for $A$ and $B$ for the four cases.\n",
    "  - Plot the marginalized posterior pdf for the signal amplitude $A$ for the four cases.\n",
    "  - Plot the marginalized posterior pdf for the background $B$ for the four cases.\n",
    "  \n",
    "  Use the same axis scales for all four cases such that the precision of the inference can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "447dc98df3d723cac0ca45562286c88d",
     "grade": false,
     "grade_id": "cell-a3772dfb9786e84d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the pdfs and combine with Bayes' theorem.\n",
    "\n",
    "def log_prior(A, B):\n",
    "    \"\"\"\n",
    "    Log prior .\n",
    "    \n",
    "    We take a uniform (flat) prior with large enough\n",
    "    maximums but, more importantly, require positive values for A and B.\n",
    "    \"\"\"\n",
    "    A_max = 5.\n",
    "    B_max = 5.\n",
    "    # flat prior \n",
    "    if np.logical_and(A <= A_max, B <= B_max).all(): \n",
    "        return np.log(1/(A_max * B_max))\n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "\n",
    "def log_likelihood(A, B, xk_pts, Nk_pts, n_0):\n",
    "    \"\"\"Log likelihood for data Nk_pts given A and B\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "def log_posterior(A, B, xk_pts, Nk_pts, n_0):\n",
    "    \"\"\"Log posterior for data Nk_pts given A and B\"\"\"\n",
    "    return log_prior(A, B) + log_likelihood(A, B, xk_pts, Nk_pts, n_0)\n",
    "\n",
    "# Other utility code can be put here (if needed)\n",
    "#\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9db3e4999485dd3db21d1fdbe4e52f0b",
     "grade": false,
     "grade_id": "cell-cbb85be603215ec4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Optional.\n",
    "# Code to find contour levels of gridded 2D posterior.\n",
    "\n",
    "def find_contour_levels(grid):\n",
    "    \"\"\"Compute 1, 2, 3-sigma contour levels for a gridded 2D posterior\n",
    "       Note: taken from BayesianAstronomy but may not work here.\n",
    "    \"\"\"\n",
    "    sorted_ = np.sort(grid.ravel())[::-1]\n",
    "    pct = np.cumsum(sorted_) / np.sum(sorted_)\n",
    "    cutoffs = np.searchsorted(pct, np.array([0.68, 0.95, 0.997]) ** 2)\n",
    "    return np.sort(sorted_[cutoffs])\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08e3b7322a9980a2b86587039488b968",
     "grade": false,
     "grade_id": "cell-219fdc3158a3dc82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Subtask b. [1 points]\n",
    "* Use the text cell below to discuss the following questions:\n",
    "  1. Can you understand why the signal and background amplitudes are anticorrelated? And why the (anti)correlation seems to be stronger in one of the cases? \n",
    "  1. Can you understand the difference in widths of the pdfs in the four cases?\n",
    "  1. What are your conclusions for how to design the experiment given limited resources? \n",
    "    - In particular, given that you wanted to be able to distinguish between signal amplitude and background, would it then be better to have many counts in few bins, or the same total amount of counts spread over a wider interval? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da8e9d825e431049ba2a471aea99a9fd",
     "grade": true,
     "grade_id": "cell-620bd3518d4f7363",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
