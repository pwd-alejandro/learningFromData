{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- See deadline on the course web page\n",
    "- This problem set is performed individually. See examination rules on the course web page.\n",
    "- Students are allowed to discuss together and help each other when solving the problems. However, every student must understand and be able to explain his/her submitted solution. Plagiarism is not allowed (submissions will be both manually and automatically monitored).\n",
    "- The two notebooks for each problem set contain a number of basic and extra problems; you can choose which and how many to work on.\n",
    "- Many problems are automatically graded using `assert` statements. You should check that your code passes these statements without raising an `AssertionError`. Note that there might be additional, hidden tests that must be passed for full credit. In addition, some tasks are either completely manually graded or part-automatic/part-manual.\n",
    "- Note that grading is performed in the teacher's python environment, which is based on the conda `environment.yml` file in the course github repo. Please avoid using additional python modules (such as `plotly`) as this might cause automatic tests to fail.\n",
    "\n",
    "- **Important:** Hand-in is performed through the following actions:\n",
    "  - Make sure to always complete **Task 0** in the header part of the notebook and that this part does not raise any `AssertionError`(s). \n",
    "  - Upload your solution in the form of your edited version of this jupyter notebook via the appropriate module in Canvas.\n",
    "  - The name of the uploaded file **must be the same as the original one**!\n",
    "  \n",
    "  Note that the hand-in might not be automatically graded if you have changed the name of the uploaded file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "- Make sure that the **run time is smaller than a few minutes**. If needed you might have to reduce some computational tasks; e.g. by decreasing the number of grid points or sampling steps. Please ask the supervisors if you are uncertain about the run time. \n",
    "\n",
    "- Your solutions are usually expected where it says `YOUR CODE HERE` or <font color=\"red\">\"PLEASE WRITE YOUR ANSWER HERE\"</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0 \n",
    "#### (0 points)\n",
    "Fill your personal details in the dictionary 'student' with the following key/value pairs:\n",
    "- **Lastname**: Your lastname as a string\n",
    "- **Firstname**: Your firstname as a string\n",
    "- **DOB-year**: The year for your date of birth as a four-digit integer\n",
    "- **DOB-month**: The month for your date of birth as an integer (1-12)\n",
    "- **DOB-day**: The year for your date of birth as an integer (1-31)\n",
    "- **CID**: Your Chalmers login ID as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e5a863952e8c2244ba81d9a82755064",
     "grade": false,
     "grade_id": "student_info",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "student={}\n",
    "# Update the values below. Note the formats.\n",
    "student['Lastname']='name_here'  # string\n",
    "student['Firstname']='name_here' # string\n",
    "student['CID']='cid_here'        # string\n",
    "student['DOB-year']=1000         # four-digit integer\n",
    "student['DOB-month']=0           # integer in the range [1, 12]\n",
    "student['DOB-day']=0             # integer in the range [1, 31]\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04fad85b593dac43e9a577b7ad0eda92",
     "grade": true,
     "grade_id": "correct_student_info",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for key in ['Lastname', 'Firstname','CID']:\n",
    "    assert type(student[key]) is str, f'{key} is wrong type.'\n",
    "    assert student[key] not in ['name_here','cid_here'],\\\n",
    "        f'Fill your {key} as a string.'\n",
    "\n",
    "for key in ['DOB-year', 'DOB-month','DOB-day']:\n",
    "    assert type(student[key]) is int, f'{key} is wrong type.'\n",
    "\n",
    "assert (student['DOB-year'] > 1900 and student['DOB-year'] < 2100)\n",
    "assert student['DOB-month'] in range(1,13), \\\n",
    "    'DOB month should be an integer in the range [1, 12]'\n",
    "assert student['DOB-day'] in range(1,32), \\\n",
    "    'DOB day should be an integer in the range [1, 31]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e21e909aaacb19cde0a49f423690276",
     "grade": false,
     "grade_id": "cell-6f99a2583f9fb27d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By changing the below boolean variable `student_self_assessment` to `True` you attest that:\n",
    "1. All handed in solutions were produced by yourself in the sense that you understand your solutions and should be able to explain and discuss them with a peer or with a teacher.\n",
    "2. That discussions with your peers are allowed, also concerning approaches to solve the problem sets, but that direct plagiarism is not allowed and that you must reach your own understanding of submitted solutions according to the definition in the previous point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3246da2eb074f5c38175d68ad158c48e",
     "grade": false,
     "grade_id": "student-self-assessment",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "student_self_assessment = False\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d873afed15d2d3de2ef460d53fccf90f",
     "grade": true,
     "grade_id": "cell-795bedd2908899aa",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert student_self_assessment == True, 'You must assert the individual solution statements.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3\n",
    "## Extra problems\n",
    "### Learning from data [TIF285], Chalmers, Fall 2022\n",
    "\n",
    "Last revised: 26-Sep-2022 by Christian ForssÃ©n [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6874d6f0612340a685cc96f6c456b90b",
     "grade": false,
     "grade_id": "cell-bf9197ccd45cb935",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Data files are stored in\n",
    "DATA_DIR = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 (extra): Bayesian optimization\n",
    "### (3 points)\n",
    "*You should have solved problem 3 to get some acquaintance with Gaussian Processes before doing this problem.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68da0c2905ed60f6f5e48980cd32dc56",
     "grade": false,
     "grade_id": "cell-9cc8731d1d9f9679",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "#\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "import GPy\n",
    "\n",
    "# Additional module import statements if needed\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A univariate minimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to minimize the function\n",
    "$$\n",
    "f(x) = \\sin(6 x) + 0.2 x^2 - 0.7 x\n",
    "$$\n",
    "on the interval $x \\in [-5,5]$.\n",
    "\n",
    "The aim is to find the position of the minimum $x^*$ to within $\\pm 0.05$ under the constraint that we would like to make as few function evaluations as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Using \"standard\" optimization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. **Plot the true function and indicate the position of the minimum**\n",
    "Save the position of the *global* minimum in the variable `xtrue_min` (with at least two significant decimals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c0cbe5fb36f692b5286cf3d2402d7a5",
     "grade": false,
     "grade_id": "cell-128fc6a755891cc9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xmin =  -5.\n",
    "xmax =  5.\n",
    "X_domain = np.linspace(xmin,xmax,10000)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b96821009c26ab47457bf13e21cefd94",
     "grade": false,
     "grade_id": "cell-3ccb1fdb17ff20ec",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "b. Find the minimum using `scipy.optimize.minimize` with `method='Nelder-Mead'`. \n",
    "* Choose the starting point randomly from a uniform pdf $U(-5,5)$. \n",
    "* Repeat one hundred times. **Do you always get the same minimum?**\n",
    "* More specifically, set the tolerance of the optimizer to `tol=0.01` and check for success by the criterion $|x^* - x^*_\\mathrm{opt}| < 0.05$, where $x^*_\\mathrm{opt}$ is the result from the optimizer.\n",
    "* Be quantitative about the average number of function evaluations that are needed per successful optimization. Compute the ratio of the total number of function evaluations number (summed over the 100 tries with different starting points) with the number of successful attempts.  \n",
    "  *Hint*: The number of function evaluations from a `scipy.optimize.minimize` result is returned in the ``OptimizeResult`` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61b0dfb83f383ea0d6f0891049c97a66",
     "grade": false,
     "grade_id": "cell-8db5c9fb60e8644c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Build your own BayesOpt algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dad0c010f8ccb5159b474ebe3588ab98",
     "grade": false,
     "grade_id": "cell-a9fb5a0414774f2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You will now implement a very different approach to minimize an objective function (this is a conventional name for the function that we are trying to minimize or maximize). The approach is known as **Bayesian optimization** and the basic idea is the following:\n",
    "* Select a few random points, evaluate the unknown function at these samples and build a **statistical model** for the function output in the entire input range based on this data (usually a Gaussian Process model).\n",
    "* Make a decision which point to sample next based on a so called **acquisition function** evaluated from the statistical model. This decision will incorporate our current knowledge about the function including our uncertainty for its value in different regions.\n",
    "* Improve the statistical model using the new sample. Continue sampling new points according to the acquisition function.\n",
    "* If done correctly, this approach will balance **exploration** of new regions (with uncertain outputs, that might contain the minimum) and **exploitation** of the region that is currently most promising.\n",
    "* Very importantly, this method also works when you are dealing with **noisy objective functions**, i.e. when your \"measurement\" of its value at a new point in parameter space contains some random noise.\n",
    "\n",
    "Your task is to repeat the above minimization with **your own Bayesian Optimization algorithm**, that should be assembled as described below. Bayesian optimization algoritms are built into libraries such as `Scikit-optimize` and `GPyOpt`, but we will build our own simple version using functions from `numpy`, `scipy`, and `GPy` (for building the statistical model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3407b7c7e50552a9011628c579b9d479",
     "grade": false,
     "grade_id": "cell-96c5e12768c3f0f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The pseudo-code for BayesOpt is the following (see specific hints for your implementation at the end):\n",
    "1. pick starting points $\\mathbf{x}^{(1)},\\mathbf{x}^{(2)},\\ldots \\mathbf{x}^{(k)}$, where $k \\geq 2$\n",
    "1. evaluate the objective function $f(\\mathbf{x})$ to obtain $y^{(i)}=f(\\mathbf{x}^{(i)})$ for $i=1,\\ldots,k$\n",
    "1. initialize a data vector $\\mathcal{D}_k = \\left\\{(\\mathbf{x}^{(i)},y^{(i)})\\right\\}_{i=1}^k$\n",
    "1. select a statistical model for $f(\\mathbf{x})$\n",
    "1. **For** {$n=k+1,k+2,\\ldots$}\n",
    "   1.    select $\\mathbf{x}^{(n)}$ by optimizing the acquisition function: $\\mathbf{x}^{(n)} = \\underset{\\mathbf{x}}{\\text{arg max}}\\, \\mathcal{A}(\\mathbf{x}|\\mathcal{D}_{n-1})$\n",
    "   1.    evaluate the objective function to obtain $y^{(n)}=f(\\mathbf{x}^{(n)})$\n",
    "   1.    augment the data vector $\\mathcal{D}_n = \\left\\{\\mathcal{D}_{n-1} , (\\mathbf{x}^{(n)},y^{(n)})\\right\\}$\n",
    "   1.    update the statistical model for $f(\\mathbf{x})$\n",
    "1. **end for**\n",
    "\n",
    "   Check for the minimum in the data vector that has been collected (note that it doesn't necessarily have to be the last sample).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1623aa6212dde7ae3920fcfac26149bd",
     "grade": false,
     "grade_id": "cell-8aca6d2119a76006",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Hints:**\n",
    "* You have to implement all steps in the above pseudo-code.\n",
    "* You can try with $k=2$ starting points.\n",
    "* For the statistical model you can use `GPy`. Follow the examples from the lectures and the exercise notebook.\n",
    "* Any knowledge about the objective function should be built into the covariance function. Let us assume that we don't have much information and that we use a standard RBF kernel.\n",
    "* It is recommended to constrain the RBF lengthscale so that it doesn't become unrealistic small. With the `GPy` model called `model`, such a constraint can be imposed using `model['rbf.lengthscale'].constrain_bounded(.1,10)`.\n",
    "* Implement the so called Lower Confidence Bound (LCB) acquisition function for use in step 5A. Then, the acquisition function is simply: $\\mathcal{A}(\\boldsymbol{x}; | \\mathcal{D}_{n-1}) = -\\mu(\\boldsymbol{x}) + \\beta \\sigma(\\boldsymbol{x})$, where\n",
    "  * $\\mu(\\boldsymbol{x})$ is the mean of the GP model trained with the data $\\mathcal{D}_{n-1})$.\n",
    "  * $\\sigma(\\boldsymbol{x})$ is the standard deviation of the GP model trained with the data $\\mathcal{D}_{n-1})$.\n",
    "  * $\\beta$ is another hyperparameter for tuning the preference for exploring unknown regions. You can set $\\beta = 2$.\n",
    "* Remember that the statistical model has to be updated (the hyperparameters re-optimized) at step 5D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6839ccea86f03ca9495f6cf4ad6a2cac",
     "grade": false,
     "grade_id": "cell-8a3750c5a21ae7bb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Tasks\n",
    "* Implement the BayesOpt minimizer\n",
    "* Assume that you are allowed a total of 100 function evaluations ($k$ of them for the starting points and $100-k$ in the loop). Are you able to find the minimum to within $\\pm 0.05$?\n",
    "* Plot the final statistical model together with the true function. Show which points that have been explored.\n",
    "* Plot also the convergence of the minimum value $\\min(y_n)$ as a function of the iteration number $n \\in \\{1, \\ldots, 100\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e5928900709f845467ebcd52887e621",
     "grade": false,
     "grade_id": "cell-51db36ede7b3bee8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the Bayesian Optimization by performing steps 1-4.\n",
    "# You might want to try with different seeds.\n",
    "#\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "616861c1702fbaf7ad5099ec2f0a6f1e",
     "grade": false,
     "grade_id": "cell-dc6717fede403c88",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform the loop, i.e. repeat steps 5A-D 100-k times\n",
    "#\n",
    "# For better performance of the BayesOptimizer it is desirable to shift the grid in each iteration\n",
    "npoints=10000\n",
    "xrange=xmax-xmin\n",
    "dx=xrange/npoints\n",
    "X_domain = np.linspace(xmin+dx*np.random.uniform(),xmax,10000).reshape(-1,1)\n",
    "\n",
    "#\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cabb813bf4af3308be50f822a11bc20",
     "grade": false,
     "grade_id": "cell-7447939914f39046",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the final result. Did you find the minimum to within 0.02?\n",
    "# Print also the hyperparameters of the final statistical model\n",
    "# (hint: `print(model)` if model is a GPy model)\n",
    "#\n",
    "# Make three plots: \n",
    "# 1. The statistical model after the final sample.\n",
    "#    Show which points that have been sampled during the run.\n",
    "# 2. The acquisition function after the final sample.\n",
    "# 3. The minimum function output as a function of the iteration number.\n",
    "#    (i.e. what is the \"best\" output that has been found so far?)  \n",
    "#    Compare to the true minimum of f(x)\n",
    "#\n",
    "#\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6 (extra): A simple Bayesian binary classifier\n",
    "### (4 points)\n",
    "*You should have solved problem 4 before doing this problem.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91c0217f52206f5f4aae950f4b4b411c",
     "grade": false,
     "grade_id": "cell-096d4fdfdae1f6b6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "#\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Additional module import statements if needed\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bayesian binary classifier that can take $(E,|m|)$ as input data and predicts a binary label (0=below $T_c$, 1=above $T_c$). \n",
    "* Use only high- and low-tempterature data for training (so that predictions for intermediate temperature data should be more difficult). Use normalized data as in Task 4.\n",
    "* The weights (and bias) of the single neuron binary classifier will be described by pdf:s that we will sample from using MCMC.\n",
    "* Use a Gaussian prior on the two weights and the bias (with ``weight decay'' $\\sigma = 1.0)$.\n",
    "* Construct the (log) likelihood as in logistic regression (i.e. as used in Task 1). \n",
    "* Use, e.g., `emcee`, for the MCMC sampling.\n",
    "* The prediction for a given input should be characterized by a pdf; i.e. the predicted probability for the state belonging to class 1 (above $T_c$) will itself be described by a pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-tasks\n",
    "(a) Set up the training data\n",
    "\n",
    "(b) Train the Bayesian binary classifier and plot the pdf:s for the weights and bias.\n",
    "\n",
    "(c) Plot the decision boundaries for a few samples of the Bayesian binary classifier. Translate to an average decision boundary.\n",
    "\n",
    "(d) Study in particular the **prediction** of your Bayesian binary classifier for inputs $(E,|m|)$ that corresponds to:\n",
    "1. low-temperature configurations.\n",
    "1. high-temperature configurations.\n",
    "1. temperatures very close to the critical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4b424e636aea94f2e329ca7208627dc",
     "grade": false,
     "grade_id": "cell-ddf368b22c78b096",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "T, Es, Ms = np.loadtxt(f'{DATA_DIR}/problem4_data.dat',unpack=True)\n",
    "\n",
    "input_data = np.column_stack((Es,Ms))\n",
    "\n",
    "Tc = 2 / np.log(1+np.sqrt(2))\n",
    "high_T = T>Tc\n",
    "\n",
    "# High-temperature = 1\n",
    "targets = high_T*np.ones_like(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf81ace6679d8ab8dc5e3c6c5f67ba9f",
     "grade": false,
     "grade_id": "cell-9b1c27024fd2e825",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Subtask (a)**: Set up the training data\n",
    "- Normalize the input data (mean=0, stddev=1)\n",
    "- Use high- (T>3.5) and low-temperature (T<1.5) data for training. \n",
    "- Plot the training data, indicate the target output 1: T>Tc with red symbols and 0: T<Tc with blue symbols.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9860fa4fe5084b6cbf525be12d64e5f4",
     "grade": false,
     "grade_id": "cell-12334284ecc2f616",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e54f79fc0290d2f7bc3df73dd99f17a",
     "grade": false,
     "grade_id": "cell-e6723fc732d3005f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Single neuron classifier code from Problem 4 is probably needed to implement the Bayesian neuron\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c756d593db690382156d51312269beec",
     "grade": false,
     "grade_id": "cell-6aaa83e747ff68ea",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the log prior, likelihood, posterior\n",
    "#\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0e258a18e4babbbf0f18ffa9a6f909f",
     "grade": false,
     "grade_id": "cell-a2016c3cbfc10935",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Sub-task (b)**: Train the Bayesian binary classifier and plot the pdf:s for the weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08f30bb79504fd8e6fa0fc27e7222cd1",
     "grade": false,
     "grade_id": "cell-b11765aaf06abae1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "922d7115e9f8de0afcdd02c8a9faaf7c",
     "grade": false,
     "grade_id": "cell-6c8a5990ef802901",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Sub-task (c)**: Plot the decision boundaries for a few samples of the Bayesian binary classifier. Translate to an average decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebf899b47873dfcd3e3054510036e83b",
     "grade": false,
     "grade_id": "cell-dc99504b5f4b570c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7fe271ac3a9243be3ab26292b75656e",
     "grade": false,
     "grade_id": "cell-b760d770900a2a3f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Sub-task (d)**: Study in particular the **prediction** of your Bayesian binary classifier for inputs $(E,|m|)$ that corresponds to:\n",
    "1. low-temperature configurations.\n",
    "1. high-temperature configurations.\n",
    "1. temperatures very close to the critical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc62241344169a5b0a25997ccad639b4",
     "grade": false,
     "grade_id": "cell-3c06bbb45fa22824",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
